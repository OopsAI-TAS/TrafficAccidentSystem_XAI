import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
from pathlib import Path
import re
from captum.attr import LayerIntegratedGradients

# ==============================================================================
# 1. ì„¤ì • ë° ëª¨ë¸ ì •ì˜ (ë‘ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ëª¨ë‘ ì •ì˜í•´ì•¼ í•¨)
# ==============================================================================
MODEL_NAME = "bert-base-multilingual-cased"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# --- [ëª¨ë¸ A] ê¸°ì¡´ í…ìŠ¤íŠ¸ ì „ìš© ëª¨ë¸ (TextOnlyHead) ---
class TextOnlyHead(nn.Module):
    def __init__(self, bert):
        super().__init__()
        self.bert = bert
        self.head = nn.Linear(bert.config.hidden_size, 2)
    
    def forward(self, input_ids, attention_mask):
        # Captum ë°°ì¹˜ ëŒ€ì‘ìš© í™•ì¥
        if input_ids is not None and attention_mask is not None:
            if attention_mask.shape[0] != input_ids.shape[0]:
                attention_mask = attention_mask.expand(input_ids.shape[0], -1)
                
        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        h_cls = out.last_hidden_state[:, 0]
        logits = self.head(h_cls)
        probs = F.softmax(logits, dim=-1)
        return probs[:, 0] * 100

# --- [ëª¨ë¸ B] ì‹ ê·œ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ (TrafficHybridSoftmax) ---
NUM_TYPE = 1000; NUM_PLACE = 500; NUM_FEAT = 500; NUM_MOVE = 300; EMBED_DIM = 32
class TrafficHybridSoftmax(nn.Module):
    def __init__(self, bert):
        super().__init__()
        self.bert = bert
        self.dropout = nn.Dropout(0.1)
        self.emb_type = nn.Embedding(NUM_TYPE, EMBED_DIM)
        self.emb_place = nn.Embedding(NUM_PLACE, EMBED_DIM)
        self.emb_feat = nn.Embedding(NUM_FEAT, EMBED_DIM)
        self.emb_a = nn.Embedding(NUM_MOVE, EMBED_DIM)
        self.emb_b = nn.Embedding(NUM_MOVE, EMBED_DIM)
        combined_dim = bert.config.hidden_size + (EMBED_DIM * 5)
        self.classifier = nn.Linear(combined_dim, 2) 

    def forward(self, input_ids, attention_mask, c_type, c_place, c_feat, c_a, c_b):
        if input_ids is not None and attention_mask is not None:
            if attention_mask.shape[0] != input_ids.shape[0]:
                attention_mask = attention_mask.expand(input_ids.shape[0], -1)

        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        h_cls = out.last_hidden_state[:, 0]
        v_type = self.emb_type(c_type); v_place = self.emb_place(c_place)
        v_feat = self.emb_feat(c_feat); v_a = self.emb_a(c_a); v_b = self.emb_b(c_b)
        combined = torch.cat([h_cls, v_type, v_place, v_feat, v_a, v_b], dim=1)
        combined = self.dropout(combined)
        logits = self.classifier(combined)
        probs = F.softmax(logits, dim=-1)
        return probs[:, 0] * 100

# ==============================================================================
# 2. ìœ í‹¸ë¦¬í‹° (íŒŒì‹± ë“±)
# ==============================================================================
def extract_code(pattern, text, max_limit):
    m = pattern.search(text)
    if m:
        val = int(m.group(1))
        return val if val < max_limit else 0
    return 0

def load_old_model(path):
    # BERTëŠ” ê³µìœ í•˜ë˜ Headë§Œ ë‹¤ë¦„
    bert = AutoModel.from_pretrained(MODEL_NAME)
    model = TextOnlyHead(bert)
    try:
        model.load_state_dict(torch.load(path))
    except:
        print("âš ï¸ ê¸°ì¡´ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨ (êµ¬ì¡° ë¶ˆì¼ì¹˜ ë“±).")
        return None
    return model.to(DEVICE).eval()

def load_new_model(path):
    bert = AutoModel.from_pretrained(MODEL_NAME)
    model = TrafficHybridSoftmax(bert)
    try:
        model.load_state_dict(torch.load(path))
    except:
        print("âš ï¸ ì‹ ê·œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨.")
        return None
    return model.to(DEVICE).eval()

# ==============================================================================
# 3. ë©”ì¸ ë¹„êµ ë¡œì§
# ==============================================================================
def main():
    # ê²½ë¡œ ì„¤ì • (ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
    PATH_OLD = Path("train/artifacts/model.pt")
    PATH_NEW = Path("train/artifacts_hybrid_softmax/best_model.pt")
    
    print("=== âš”ï¸ Model XAI Battle: Old vs New âš”ï¸ ===")
    
    tok = AutoTokenizer.from_pretrained(MODEL_NAME)
    
    # ëª¨ë¸ ë¡œë“œ
    print("1. Loading Old Model (Text Only)...")
    model_old = load_old_model(PATH_OLD)
    
    print("2. Loading New Model (Hybrid)...")
    model_new = load_new_model(PATH_NEW)
    
    if model_old is None or model_new is None:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # â˜… ë¹„êµí•  í…ŒìŠ¤íŠ¸ ë¬¸ì¥ (ê°€ì¥ ì „í˜•ì ì¸ ì¼€ì´ìŠ¤)
    text = "[ì‚¬ê³  ì •ë³´] ì‚¬ê³ ìœ í˜•=ì§ì§„ëŒ€ì¢ŒíšŒì „(ì½”ë“œ=214), ì‚¬ê³ ì¥ì†Œ=êµì°¨ë¡œ(ì½”ë“œ=25) [ì°¨ëŸ‰ ì§„í–‰] Aì°¨ëŸ‰=ì§ì§„(ì½”ë“œ=6), Bì°¨ëŸ‰=ì¢ŒíšŒì „(ì½”ë“œ=14)"
    
    print(f"\n[Input Text]\n{text}")
    print("-" * 60)

    # ---------------------------------------------------------
    # [ë¶„ì„ 1] Old Model (Text Only)
    # ---------------------------------------------------------
    enc = tok(text, return_tensors="pt", max_length=256, padding="max_length", truncation=True)
    ids = enc["input_ids"].to(DEVICE)
    mask = enc["attention_mask"].to(DEVICE)
    
    print("\nğŸ•µï¸â€â™‚ï¸ [Old Model Analysis]")
    with torch.no_grad():
        pred_old = model_old(ids, mask).item()
    print(f"ğŸ‘‰ ì˜ˆì¸¡ê°’: {pred_old:.2f} %")
    
    lig_old = LayerIntegratedGradients(model_old, model_old.bert.embeddings)
    attr_old = lig_old.attribute(inputs=ids, additional_forward_args=(mask,), n_steps=50)
    score_old = attr_old.sum(dim=2).squeeze(0)
    score_old = score_old / torch.norm(score_old)
    
    # ---------------------------------------------------------
    # [ë¶„ì„ 2] New Model (Hybrid)
    # ---------------------------------------------------------
    # ì •ê·œì‹ íŒŒì‹±
    pat_type = re.compile(r"ì‚¬ê³ ìœ í˜•=[^,]*?\(?(?:ì½”ë“œ=)?(\d+)\)?")
    pat_place = re.compile(r"ì‚¬ê³ ì¥ì†Œ=[^,]*?\(?(?:ì½”ë“œ=)?(\d+)\)?")
    pat_feat  = re.compile(r"ì¥ì†ŒíŠ¹ì§•=[^,]*?\(?(?:ì½”ë“œ=)?(\d+)\)?")
    pat_a     = re.compile(r"Aì°¨ëŸ‰[^,]*?\(?(?:ì½”ë“œ=)?(\d+)\)?")
    pat_b     = re.compile(r"Bì°¨ëŸ‰[^,]*?\(?(?:ì½”ë“œ=)?(\d+)\)?")
    
    c_type = torch.tensor([extract_code(pat_type, text, 1000)]).to(DEVICE)
    c_place = torch.tensor([extract_code(pat_place, text, 500)]).to(DEVICE)
    c_feat = torch.tensor([extract_code(pat_feat, text, 500)]).to(DEVICE)
    c_a = torch.tensor([extract_code(pat_a, text, 300)]).to(DEVICE)
    c_b = torch.tensor([extract_code(pat_b, text, 300)]).to(DEVICE)
    
    print("\nğŸ•µï¸â€â™‚ï¸ [New Model Analysis]")
    with torch.no_grad():
        pred_new = model_new(ids, mask, c_type, c_place, c_feat, c_a, c_b).item()
    print(f"ğŸ‘‰ ì˜ˆì¸¡ê°’: {pred_new:.2f} %")
    
    lig_new = LayerIntegratedGradients(model_new, [
        model_new.bert.embeddings, model_new.emb_type, model_new.emb_place, model_new.emb_a, model_new.emb_b
    ])
    attr_new = lig_new.attribute(inputs=(ids, mask, c_type, c_place, c_feat, c_a, c_b), n_steps=50)
    
    score_new_text = attr_new[0].sum(dim=2).squeeze(0)
    score_new_text = score_new_text / torch.norm(score_new_text)
    
    # ---------------------------------------------------------
    # [ê²°ê³¼ ì¶œë ¥] Side-by-Side ë¹„êµ
    # ---------------------------------------------------------
    print("\n" + "="*70)
    print(f"{'Token':<12} | {'Old Model Score':<15} | {'New Model Score':<15}")
    print("="*70)
    
    tokens = tok.convert_ids_to_tokens(ids[0])
    for t, s1, s2 in zip(tokens, score_old, score_new_text):
        if t == "[PAD]": break
        # ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ì˜ë¯¸ ìˆê²Œ ë´¤ìœ¼ë©´ ì¶œë ¥ (ì ˆëŒ“ê°’ 0.05 ì´ìƒ)
        if abs(s1.item()) > 0.05 or abs(s2.item()) > 0.05:
            print(f"{t:<12} | {s1.item():15.4f} | {s2.item():15.4f}")
            
    print("-" * 70)
    print("\n[New Model Special Features (Code Importance)]")
    print(f"Code A (ì§ì§„)   Impact: {attr_new[3].sum().item():.4f}")
    print(f"Code B (ì¢ŒíšŒì „) Impact: {attr_new[4].sum().item():.4f}")
    print(f"Code Type      Impact: {attr_new[1].sum().item():.4f}")

if __name__ == "__main__":
    main()