import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
from pathlib import Path
import re
from captum.attr import LayerIntegratedGradients

# ==========================================
# 1. ëª¨ë¸ ì •ì˜ (Mask í™•ì¥ ê¸°ëŠ¥ í¬í•¨)
# ==========================================
MODEL_NAME = "bert-base-multilingual-cased"
NUM_TYPE = 1000; NUM_PLACE = 500; NUM_FEAT = 500; NUM_MOVE = 300; EMBED_DIM = 32

class TrafficHybridSoftmax(nn.Module):
    def __init__(self, bert):
        super().__init__()
        self.bert = bert
        self.dropout = nn.Dropout(0.1)
        self.emb_type = nn.Embedding(NUM_TYPE, EMBED_DIM)
        self.emb_place = nn.Embedding(NUM_PLACE, EMBED_DIM)
        self.emb_feat = nn.Embedding(NUM_FEAT, EMBED_DIM)
        self.emb_a = nn.Embedding(NUM_MOVE, EMBED_DIM)
        self.emb_b = nn.Embedding(NUM_MOVE, EMBED_DIM)
        
        combined_dim = bert.config.hidden_size + (EMBED_DIM * 5)
        self.classifier = nn.Linear(combined_dim, 2) 

    def forward(self, input_ids, attention_mask, c_type, c_place, c_feat, c_a, c_b):
        # [Captum ë°°ì¹˜ì²˜ë¦¬ ëŒ€ì‘] Mask ìë™ í™•ì¥
        if input_ids is not None and attention_mask is not None:
            batch_size = input_ids.shape[0]
            if attention_mask.shape[0] != batch_size:
                attention_mask = attention_mask.expand(batch_size, -1)

        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        h_cls = out.last_hidden_state[:, 0]
        
        v_type = self.emb_type(c_type); v_place = self.emb_place(c_place)
        v_feat = self.emb_feat(c_feat); v_a = self.emb_a(c_a); v_b = self.emb_b(c_b)
        
        combined = torch.cat([h_cls, v_type, v_place, v_feat, v_a, v_b], dim=1)
        combined = self.dropout(combined)
        logits = self.classifier(combined)
        
        probs = F.softmax(logits, dim=-1)
        return probs[:, 0] * 100

# ==========================================
# 2. ì‹¤í–‰ ë° ë¶„ì„ ë¡œì§
# ==========================================
def extract_code(pattern, text, max_limit):
    m = pattern.search(text)
    if m:
        val = int(m.group(1))
        return val if val < max_limit else 0
    return 0

def main():
    CKPT = Path("train/artifacts_hybrid_softmax")
    model_path = CKPT / "best_model.pt"
    
    if not model_path.exists():
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        return

    print(f"Loading model from {model_path}...")
    tok = AutoTokenizer.from_pretrained(str(CKPT))
    bert = AutoModel.from_pretrained(str(CKPT))
    model = TrafficHybridSoftmax(bert)
    model.load_state_dict(torch.load(model_path))
    model.eval().cuda()

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    text = "[ì‚¬ê³  ì •ë³´] ì‚¬ê³ ìœ í˜•=ì§ì§„ëŒ€ì¢ŒíšŒì „(ì½”ë“œ=214), ì‚¬ê³ ì¥ì†Œ=êµì°¨ë¡œ(ì½”ë“œ=25) [ì°¨ëŸ‰ ì§„í–‰] Aì°¨ëŸ‰=ì§ì§„(ì½”ë“œ=6), Bì°¨ëŸ‰=ì¢ŒíšŒì „(ì½”ë“œ=14)"
    
    # Regex
    pat_type = re.compile(r"ì‚¬ê³ ìœ í˜•=[^,]*?\(?(?:ì½”ë“œ=)?(\d+)\)?")
    pat_place = re.compile(r"ì‚¬ê³ ì¥ì†Œ=[^,]*?\(?(?:ì½”ë“œ=)?(\d+)\)?")
    pat_feat  = re.compile(r"ì¥ì†ŒíŠ¹ì§•=[^,]*?\(?(?:ì½”ë“œ=)?(\d+)\)?")
    pat_a     = re.compile(r"Aì°¨ëŸ‰[^,]*?\(?(?:ì½”ë“œ=)?(\d+)\)?")
    pat_b     = re.compile(r"Bì°¨ëŸ‰[^,]*?\(?(?:ì½”ë“œ=)?(\d+)\)?")

    # Input ì¤€ë¹„
    enc = tok(text, return_tensors="pt", max_length=256, padding="max_length", truncation=True)
    input_ids = enc["input_ids"].cuda()
    mask = enc["attention_mask"].cuda() # ë³€ìˆ˜ëª…: mask
    
    c_type = torch.tensor([extract_code(pat_type, text, NUM_TYPE)]).cuda()
    c_place = torch.tensor([extract_code(pat_place, text, NUM_PLACE)]).cuda()
    c_feat = torch.tensor([extract_code(pat_feat, text, NUM_FEAT)]).cuda()
    c_a = torch.tensor([extract_code(pat_a, text, NUM_MOVE)]).cuda()
    c_b = torch.tensor([extract_code(pat_b, text, NUM_MOVE)]).cuda()

    print(f"\nì…ë ¥: {text}")
    print(f"íŒŒì‹±: Type={c_type.item()}, Place={c_place.item()}, A={c_a.item()}, B={c_b.item()}")

    # ì˜ˆì¸¡
    with torch.no_grad():
        pred = model(input_ids, mask, c_type, c_place, c_feat, c_a, c_b)
    print(f"ì˜ˆì¸¡ ê³¼ì‹¤ë¹„ìœ¨(A): {pred.item():.2f}")

    # IG ë¶„ì„
    print("\nCalculating Feature Importance (this may take a moment)...")
    lig = LayerIntegratedGradients(model, [
        model.bert.embeddings, 
        model.emb_type, model.emb_place, model.emb_a, model.emb_b
    ])
    
    # ğŸ”´ [ìˆ˜ì •ë¨] inputs íŠœí”Œ ì•ˆì˜ ë³€ìˆ˜ëª…ì„ 'mask'ë¡œ ë³€ê²½
    attr = lig.attribute(
        inputs=(input_ids, mask, c_type, c_place, c_feat, c_a, c_b), 
        n_steps=50
    )
    
    # ê²°ê³¼ ì •ë¦¬
    text_attr = attr[0].sum(dim=2).squeeze(0)
    text_attr = text_attr / torch.norm(text_attr)
    
    score_type = attr[1].sum().item()
    score_place = attr[2].sum().item()
    score_a = attr[3].sum().item()
    score_b = attr[4].sum().item()

    print("\n[ğŸ“Š Feature Importance Comparison]")
    print("-" * 45)
    print(f"{'Source':<15} | {'Value':<10} | {'Importance':<10}")
    print("-" * 45)
    print(f"{'Code: A-Move':<15} | {c_a.item():<10} | {score_a:.4f}")
    print(f"{'Code: B-Move':<15} | {c_b.item():<10} | {score_b:.4f}")
    print(f"{'Code: Type':<15} | {c_type.item():<10} | {score_type:.4f}")
    print(f"{'Code: Place':<15} | {c_place.item():<10} | {score_place:.4f}")
    print("-" * 45)
    
    print("\n[ğŸ“ Top Text Tokens (BERT)]")
    tokens = tok.convert_ids_to_tokens(input_ids[0])
    print(f"{'Token':<15} | {'Score':<10}")
    print("-" * 30)
    
    for t, s in zip(tokens, text_attr):
        if t == "[PAD]": break
        if abs(s.item()) > 0.03: 
            print(f"{t:<15} | {s.item():.4f}")

if __name__ == "__main__":
    main()